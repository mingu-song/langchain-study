{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search langchain-community\n",
    "!pip install -qU faiss-cpu pymupdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"agentic-rag-{uuid4().hex[0:8]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "docs = ArxivLoader(query=\"Retrieval Augmented Generation\", max_results=5).load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = faiss_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following context to answer the user's query in Korean. If you cannot answer the question, please respond with 'I don't know'.\\n\\nQuestion:\\n{question}\\n\\nContext:\\n{context}\\n\"))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = \"\"\"\\\n",
    "Use the following context to answer the user's query in Korean. If you cannot answer the question, please respond with 'I don't know'.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(rag_prompt)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "open_chat_model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(itemgetter('question'))\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000012A07280340>),\n",
       "  question: RunnableLambda(itemgetter('question'))\n",
       "}\n",
       "| RunnableAssign(mapper={\n",
       "    context: RunnableLambda(itemgetter('context'))\n",
       "  })\n",
       "| {\n",
       "    response: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following context to answer the user's query in Korean. If you cannot answer the question, please respond with 'I don't know'.\\n\\nQuestion:\\n{question}\\n\\nContext:\\n{context}\\n\"))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000012A07828E80>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000012A41612C50>, model_name='gpt-4o', openai_api_key=SecretStr('**********'), openai_proxy=''),\n",
       "    context: RunnableLambda(itemgetter('context'))\n",
       "  }"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\") }\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": template | open_chat_model, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': AIMessage(content='Retrieval-Augmented Generation (RAG)는 정보 검색 과정을 도입하여 생성 프로세스를 향상시키는 패러다임입니다. RAG는 외부 데이터 소스에서 관련 정보를 검색한 후 이를 생성 모델과 상호작용시켜 전체 생성 과정을 강화합니다. RAG는 다양한 방법으로 정보를 검색하고 생성 프로세스에 통합하여 정확성과 견고성을 높일 수 있습니다. 예를 들어, RAG는 대화 응답 생성, 기계 번역, 요약 생성 등의 다양한 작업에 적용될 수 있습니다. RAG는 또한 큰 생성 모델의 크기를 줄이고, 긴 문맥을 지원하며, 특정 생성 단계를 제거하여 비용을 절감하는 데 도움을 줄 수 있습니다.', response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 2544, 'total_tokens': 2700}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_729ea513f7', 'finish_reason': 'stop', 'logprobs': None}, id='run-8c6a17bb-d143-451c-af30-c5b5e40d2928-0'),\n",
       " 'context': [Document(page_content='grating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications\\nof retrieval-augmented generation in other genera-\\ntion tasks such as abstractive summarization (Peng\\net al., 2019), code generation (Hashimoto et al.,\\n2018), paraphrase (Kazemnejad et al., 2020; Su\\net al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2\\nRetrieval-Augmented Paradigm\\nIn this section, we ﬁrst give a general formulation\\nof retrieval-augmented text generation. Then, we\\ndiscuss three major components of the retrieval-\\naugmented generation paradigm, including the re-\\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\\nInput\\nSources \\n(Sec. 2.2):\\nTraining \\nCorpus\\nExternal Data\\nUnsupervised \\nData\\nMetrics\\n(Sec. 2.3):\\nSparse-vector \\nRetrieval\\nDense-vector \\nRetrieval\\nTask-specific \\nRetrieval\\nRetrieval Memory\\nGeneration Model\\nSec. 4: Machine \\nTranslation\\nSec. 5: Other \\nTasks\\nData \\nAugmentation\\nAttention \\nMechanism\\nSkeleton & \\nTemplates\\nInformation Retrieval', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}),\n",
       "  Document(page_content='Retrieval-Augmented Generation (RAG) is proposed to allevi-\\nate, if not completely address, the aforementioned challenges\\nthrough its adaptable data repository [29]. The knowledge\\nstored for retrieval can be conceptualized as non-parametric\\nmemory, which is easily modifiable, capable of accommo-\\ndating broad long-tail knowledge, and also able to encode\\nconfidential data. In addition, retrieval can also be employed\\nto reduce the generation costs. For example, RAG can reduce\\nthe size of large generative models [30], provide support for\\nlong contexts [31], and eliminate certain generation steps [32].\\nA typical RAG process is depicted in Fig. 1. Given an\\ninput query, the retriever locates and looks up relevant data\\nsources, then the retrieved results interact with the generator\\nto enhance the overall generation process. There are sev-\\neral foundational paradigms (foundations in short) according\\nto how the retrieved results augment the generation: they\\ncan serve as augmented input to the generator [33], [34];\\nthey can join at the middle stage of generation as latent\\nrepresentations [35], [36]; they can contribute to the final\\ngeneration results in the form of logits [37], [38]; they can\\neven influence or omit certain generation steps [32], [39].\\narXiv:2402.19473v4  [cs.CV]  2 May 2024\\n2\\nFig. 1: A generic RAG architecture. The user queries, spanning\\ndifferent modalities, serve as input to both the retriever and', metadata={'Published': '2024-05-02', 'Title': 'Retrieval-Augmented Generation for AI-Generated Content: A Survey', 'Authors': 'Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui', 'Summary': 'Advancements in model algorithms, the growth of foundational models, and\\naccess to high-quality datasets have propelled the evolution of Artificial\\nIntelligence Generated Content (AIGC). Despite its notable successes, AIGC\\nstill faces hurdles such as updating knowledge, handling long-tail data,\\nmitigating data leakage, and managing high training and inference costs.\\nRetrieval-Augmented Generation (RAG) has recently emerged as a paradigm to\\naddress such challenges. In particular, RAG introduces the information\\nretrieval process, which enhances the generation process by retrieving relevant\\nobjects from available data stores, leading to higher accuracy and better\\nrobustness. In this paper, we comprehensively review existing efforts that\\nintegrate RAG technique into AIGC scenarios. We first classify RAG foundations\\naccording to how the retriever augments the generator, distilling the\\nfundamental abstractions of the augmentation methodologies for various\\nretrievers and generators. This unified perspective encompasses all RAG\\nscenarios, illuminating advancements and pivotal technologies that help with\\npotential future progress. We also summarize additional enhancements methods\\nfor RAG, facilitating effective engineering and implementation of RAG systems.\\nThen from another view, we survey on practical applications of RAG across\\ndifferent modalities and tasks, offering valuable references for researchers\\nand practitioners. Furthermore, we introduce the benchmarks for RAG, discuss\\nthe limitations of current RAG systems, and suggest potential directions for\\nfuture research. Github: https://github.com/PKU-DAIR/RAG-Survey.'}),\n",
       "  Document(page_content='Active Retrieval Augmented Generation\\nZhengbao Jiang1∗\\nFrank F. Xu1∗\\nLuyu Gao1∗\\nZhiqing Sun1∗\\nQian Liu2\\nJane Dwivedi-Yu3\\nYiming Yang1\\nJamie Callan1\\nGraham Neubig1\\n1Language Technologies Institute, Carnegie Mellon University\\n2Sea AI Lab\\n3FAIR, Meta\\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\\nAbstract\\nDespite the remarkable ability of large lan-\\nguage models (LMs) to comprehend and gen-\\nerate language, they have a tendency to hal-\\nlucinate and create factually inaccurate out-\\nput. Augmenting LMs by retrieving informa-\\ntion from external knowledge resources is one\\npromising solution. Most existing retrieval aug-\\nmented LMs employ a retrieve-and-generate\\nsetup that only retrieves information once based\\non the input.\\nThis is limiting, however, in\\nmore general scenarios involving generation\\nof long texts, where continually gathering in-\\nformation throughout generation is essential. In\\nthis work, we provide a generalized view of ac-\\ntive retrieval augmented generation, methods\\nthat actively decide when and what to retrieve\\nacross the course of the generation. We propose\\nForward-Looking Active REtrieval augmented\\ngeneration (FLARE), a generic method which\\niteratively uses a prediction of the upcoming\\nsentence to anticipate future content, which is\\nthen utilized as a query to retrieve relevant doc-', metadata={'Published': '2023-10-22', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing retrieval\\naugmented LMs employ a retrieve-and-generate setup that only retrieves\\ninformation once based on the input. This is limiting, however, in more general\\nscenarios involving generation of long texts, where continually gathering\\ninformation throughout generation is essential. In this work, we provide a\\ngeneralized view of active retrieval augmented generation, methods that\\nactively decide when and what to retrieve across the course of the generation.\\nWe propose Forward-Looking Active REtrieval augmented generation (FLARE), a\\ngeneric method which iteratively uses a prediction of the upcoming sentence to\\nanticipate future content, which is then utilized as a query to retrieve\\nrelevant documents to regenerate the sentence if it contains low-confidence\\ntokens. We test FLARE along with baselines comprehensively over 4 long-form\\nknowledge-intensive generation tasks/datasets. FLARE achieves superior or\\ncompetitive performance on all tasks, demonstrating the effectiveness of our\\nmethod. Code and datasets are available at https://github.com/jzbjyb/FLARE.'}),\n",
       "  Document(page_content='augmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about\\nretrieval-augmented generation, which are orga-\\nnized with respect to different tasks. Speciﬁcally,\\non the dialogue response generation task, exem-\\nplar/template retrieval as an intermediate step has\\nbeen shown beneﬁcial to informative response gen-\\neration (Weston et al., 2018; Wu et al., 2019; Cai\\net al., 2019a,b). In addition, there has been growing\\ninterest in knowledge-grounded generation explor-\\ning different forms of knowledge such as knowl-\\nedge bases and external documents (Dinan et al.,\\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\\n2021). On the machine translation task, we summa-\\nrize the early work on how the retrieved sentences\\n(called translation memory) are used to improve\\nstatistical machine translation (SMT) (Koehn et al.,\\n2003) models (Simard and Isabelle, 2009; Koehn\\nand Senellart, 2010) and in particular, we inten-\\nsively highlight several popular methods to inte-\\ngrating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.ainvoke({\"question\": \"What is Retrieval Augmented Generation?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
