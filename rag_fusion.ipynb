{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain + RAG Fusion + GPT-4o Python Project: Easy AI/Chat for your Docs\n",
    "* https://medium.com/gitconnected/langchain-rag-fusion-gpt-4o-python-project-easy-ai-chat-for-your-docs-1b802f889318\n",
    "\n",
    "![](./img/rag-fusion.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_MODEL_OPENAI = \"gpt-4o\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "TOP_K = 5\n",
    "MAX_DOCS_FOR_CONTEXT = 8\n",
    "DOCUMENT_PDF = \"./data/gs.pdf\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_document(pdf: str) -> list[Document]:\n",
    "    raw_documents = PyPDFLoader(pdf).load()\n",
    "    text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "    documents = text_splitter.split_documents(raw_documents)\n",
    "    print(\"Original document: \", len(documents), \" docs\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(search_type: str, kwargs: dict) -> BaseRetriever:\n",
    "    documents = load_and_split_document(DOCUMENT_PDF)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "    store = LocalFileStore(\"./data/cache/\")\n",
    "    cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "        embeddings,\n",
    "        store,\n",
    "        namespace=embeddings.model\n",
    "    )\n",
    "    vectordb = Chroma.from_documents(documents, cached_embedder)\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs=kwargs,\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로, 우리는 각 쿼리에 대해 유사한 문서를 찾기 위해 벡터 검색을 수행하고, 그런 다음 이러한 문서들을 재정렬합니다. 재정렬을 위해 우리는 유사도 순위에 의존하는 Reciprocal Rank Fusion (RRF)을 사용합니다.\n",
    "\n",
    "Reciprocal Rank Fusion에서 문서 d의 점수 𝑅𝑅𝐹(𝑑)은 문서 𝑑의 순위인 𝑟𝑎𝑛𝑘 𝑖(𝑑)와 하이퍼파라미터 k를 기반으로 계산됩니다.\n",
    "\n",
    "하이퍼파라미터 k는 Reciprocal Rank Fusion (RRF) 점수에 영향을 미칩니다. k 값이 높을수록 점수 곡선이 평평해져 낮은 순위의 문서들이 더 큰 영향을 받게 됩니다. 일반적으로 k=60이 자주 사용되며, 나는 현재 분석에서 이 값을 채택하고 있습니다.\n",
    "\n",
    "Reciprocal Rank Fusion을 계산하는 함수는 reciprocal_rank_fusion이라고 합니다. 테스트 목적으로 RRF 점수를 표시하지만, 문서 조각의 내용만이 LLM에게 맥락으로 전달되어야 하기 때문에 반환하는 것은 문서 조각 목록뿐입니다.\n",
    "\n",
    "네 개의 유사한 쿼리 각각이 다섯 개의 조각을 검색하기 때문에, 총 조각 수는 최대 20개에 이를 수 있습니다. 그러나 이는 과도한 맥락을 생성하므로 상위 순위의 조각들만 사용됩니다. 맥락으로 전달되는 문서의 최대 수, MAX_DOCS_FOR_CONTEXT는 8로 설정되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1 / (rank + k )\n",
    "    \n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    print(\"Reranked documents: \", len(reranked_results))\n",
    "    for doc in reranked_results:\n",
    "        print('---')\n",
    "        print('Docs: ', ' '.join(doc[0].page_content[:100].split()))\n",
    "        print('RRF score: ', doc[1])\n",
    "\n",
    "    return [x[0] for x in reranked_results[:MAX_DOCS_FOR_CONTEXT]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 함수인 query_generator는 유사한 쿼리를 생성합니다. 이 함수는 원래의 쿼리인 original_query를 프롬프트에 삽입하고 LLM(언어 모델)에게 유사한 쿼리를 생성하도록 요청함으로써 작동합니다. 원래 코드에서는 단순히 'Generate multiple search queries related to: {original_query}'라는 프롬프트만 사용했습니다. 이 접근 방식은 비교적 광범위한 쿼리를 생성했습니다.\n",
    "\n",
    "결과를 좁히기 위해, 나는 프롬프트에 다음과 같은 지시문을 추가하도록 업데이트했습니다: 'When creating queries, please refine or add closely related contextual information, without significantly altering the original query's meaning.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_generator(original_query: dict) -> list[str]:\n",
    "    query = original_query.get(\"query\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"),\n",
    "        (\"user\", \"Generate mnultiple search queries related to: {original_query}. When creating queries, please refine or add closely related contextual information in Korean, without significantly altering the original query's meaning\"),\n",
    "        (\"user\", \"OUTPUT (3 queries):\"),\n",
    "    ])\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=LLM_MODEL_OPENAI)\n",
    "\n",
    "    query_generator_chain = (\n",
    "        prompt | model | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "    )\n",
    "\n",
    "    queries = query_generator_chain.invoke({\"original_query\": query})\n",
    "\n",
    "    queries.insert(0, \"0. \" + query)\n",
    "\n",
    "    print(\"Generated queries:\\n\", \"\\n\".join(queries))\n",
    "\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터 검색은 다음과 같이 수행됩니다:\n",
    "\n",
    "1. RunnableLambda(query_generator)는 앞서 설명한 대로 유사한 쿼리를 생성합니다.\n",
    "2. retriever.map()을 사용하여 query_generator가 생성한 네 개의 유사한 쿼리와 원래 쿼리를 포함하여 각 쿼리에 대해 벡터 검색을 수행합니다. map() 함수는 각 쿼리에 대해 다섯 개의 조각을 검색합니다.\n",
    "3. 마지막으로, reciprocal_rank_fusion을 적용하여 결과를 재정렬합니다. 이는 다음 섹션에서 자세히 다룰 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_retriever(query: str) -> list[Document]:\n",
    "    retriever = create_retriever(search_type=\"similarity\", kwargs={\"k\": TOP_K})\n",
    "\n",
    "    chain = (\n",
    "        {\"query\": itemgetter(\"query\")}\n",
    "        | RunnableLambda(query_generator)\n",
    "        | retriever.map()\n",
    "        | reciprocal_rank_fusion\n",
    "    )\n",
    "\n",
    "    result = chain.invoke({\"query\": query})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query: str):\n",
    "    model = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model=LLM_MODEL_OPENAI\n",
    "    )\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Please answer the [question] using only the following [information] in Korean. If there is no [information] available to answer the question, do not force an answer.\n",
    "\n",
    "            Information: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            Final answer:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | RunnableLambda(rrf_retriever),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | RunnablePassthrough.assign(\n",
    "            context=itemgetter(\"context\")\n",
    "        )\n",
    "        | {\n",
    "            \"response\": prompt | model | StrOutputParser(),\n",
    "            \"context\": itemgetter(\"context\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = chain.invoke({\"question\": query})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document:  11  docs\n",
      "Generated queries:\n",
      " 0. 개인퇴직연금\n",
      "1. 개인퇴직연금 가입 방법 및 절차\n",
      "2. 개인퇴직연금 세제 혜택과 절세 방법\n",
      "3. 개인퇴직연금 운용 전략 및 추천 상품\n",
      "Reranked documents:  3\n",
      "---\n",
      "Docs:  10/118. 상해보험 1)가입대상 : 전 직원 (임원상해보험 별도 가입) 2) 가입내용 : 全 직원의 불의의 사고 및 질병에 대비하기 위해 회사에서 일괄적으로\n",
      "RRF score:  0.16137431484139575\n",
      "---\n",
      "Docs:  10. 주택 자금 대출 제도 1)대출 내용 : 본인의 거주 목적으로 주택 구입 혹은 임차 계약 시 대출 지원 (무주택자에 한함) 2)대출 한도 : 5천만원 3)대출 금리 :\n",
      "RRF score:  0.08068715742069787\n",
      "---\n",
      "Docs:  구 분경조금 (월기본급 기준)휴가화환장례용품 장례인력 결혼본인 100% 5일○ 자녀 100% 2일○ 형제·자매(본인,배우자 ) 50% 1일 회갑본인/배우자 100% 1일 부모(본\n",
      "RRF score:  0.08068715742069787\n",
      "---\n",
      "Answer:개인퇴직연금에 대한 정보는 다음과 같습니다:\n",
      "\n",
      "1) 지원 내용 : 임직원의 노후 생활 안정을 위하여 개인퇴직연금 지원\n",
      "2) 지원 금액 : 임직원이 적립하는 금액과 동일하게 회사 지원분 적립함\n",
      "단, 최대 月 25만원 한도로 지원함\n"
     ]
    }
   ],
   "source": [
    "result = query(\"개인퇴직연금\")\n",
    "print('---\\nAnswer:' + result['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
