{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain + RAG Fusion + GPT-4o Python Project: Easy AI/Chat for your Docs\n",
    "* https://medium.com/gitconnected/langchain-rag-fusion-gpt-4o-python-project-easy-ai-chat-for-your-docs-1b802f889318\n",
    "\n",
    "![](./img/rag-fusion.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.load import dumps, loads\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_MODEL_OPENAI = \"gpt-4o\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "TOP_K = 5\n",
    "MAX_DOCS_FOR_CONTEXT = 8\n",
    "DOCUMENT_PDF = \"./data/gs.pdf\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_document(pdf: str) -> list[Document]:\n",
    "    raw_documents = PyPDFLoader(pdf).load()\n",
    "    text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "    documents = text_splitter.split_documents(raw_documents)\n",
    "    print(\"Original document: \", len(documents), \" docs\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(search_type: str, kwargs: dict) -> BaseRetriever:\n",
    "    documents = load_and_split_document(DOCUMENT_PDF)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "    store = LocalFileStore(\"./data/cache/\")\n",
    "    cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "        embeddings,\n",
    "        store,\n",
    "        namespace=embeddings.model\n",
    "    )\n",
    "    vectordb = Chroma.from_documents(documents, cached_embedder)\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs=kwargs,\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ, ìš°ë¦¬ëŠ” ê° ì¿¼ë¦¬ì— ëŒ€í•´ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•´ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê·¸ëŸ° ë‹¤ìŒ ì´ëŸ¬í•œ ë¬¸ì„œë“¤ì„ ì¬ì •ë ¬í•©ë‹ˆë‹¤. ì¬ì •ë ¬ì„ ìœ„í•´ ìš°ë¦¬ëŠ” ìœ ì‚¬ë„ ìˆœìœ„ì— ì˜ì¡´í•˜ëŠ” Reciprocal Rank Fusion (RRF)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "Reciprocal Rank Fusionì—ì„œ ë¬¸ì„œ dì˜ ì ìˆ˜ ğ‘…ğ‘…ğ¹(ğ‘‘)ì€ ë¬¸ì„œ ğ‘‘ì˜ ìˆœìœ„ì¸ ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘–(ğ‘‘)ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° kë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì´í¼íŒŒë¼ë¯¸í„° këŠ” Reciprocal Rank Fusion (RRF) ì ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. k ê°’ì´ ë†’ì„ìˆ˜ë¡ ì ìˆ˜ ê³¡ì„ ì´ í‰í‰í•´ì ¸ ë‚®ì€ ìˆœìœ„ì˜ ë¬¸ì„œë“¤ì´ ë” í° ì˜í–¥ì„ ë°›ê²Œ ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ k=60ì´ ìì£¼ ì‚¬ìš©ë˜ë©°, ë‚˜ëŠ” í˜„ì¬ ë¶„ì„ì—ì„œ ì´ ê°’ì„ ì±„íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Reciprocal Rank Fusionì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ëŠ” reciprocal_rank_fusionì´ë¼ê³  í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ RRF ì ìˆ˜ë¥¼ í‘œì‹œí•˜ì§€ë§Œ, ë¬¸ì„œ ì¡°ê°ì˜ ë‚´ìš©ë§Œì´ LLMì—ê²Œ ë§¥ë½ìœ¼ë¡œ ì „ë‹¬ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë°˜í™˜í•˜ëŠ” ê²ƒì€ ë¬¸ì„œ ì¡°ê° ëª©ë¡ë¿ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë„¤ ê°œì˜ ìœ ì‚¬í•œ ì¿¼ë¦¬ ê°ê°ì´ ë‹¤ì„¯ ê°œì˜ ì¡°ê°ì„ ê²€ìƒ‰í•˜ê¸° ë•Œë¬¸ì—, ì´ ì¡°ê° ìˆ˜ëŠ” ìµœëŒ€ 20ê°œì— ì´ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” ê³¼ë„í•œ ë§¥ë½ì„ ìƒì„±í•˜ë¯€ë¡œ ìƒìœ„ ìˆœìœ„ì˜ ì¡°ê°ë“¤ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ë§¥ë½ìœ¼ë¡œ ì „ë‹¬ë˜ëŠ” ë¬¸ì„œì˜ ìµœëŒ€ ìˆ˜, MAX_DOCS_FOR_CONTEXTëŠ” 8ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1 / (rank + k )\n",
    "    \n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    print(\"Reranked documents: \", len(reranked_results))\n",
    "    for doc in reranked_results:\n",
    "        print('---')\n",
    "        print('Docs: ', ' '.join(doc[0].page_content[:100].split()))\n",
    "        print('RRF score: ', doc[1])\n",
    "\n",
    "    return [x[0] for x in reranked_results[:MAX_DOCS_FOR_CONTEXT]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒ í•¨ìˆ˜ì¸ query_generatorëŠ” ìœ ì‚¬í•œ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì›ë˜ì˜ ì¿¼ë¦¬ì¸ original_queryë¥¼ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•˜ê³  LLM(ì–¸ì–´ ëª¨ë¸)ì—ê²Œ ìœ ì‚¬í•œ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•¨ìœ¼ë¡œì¨ ì‘ë™í•©ë‹ˆë‹¤. ì›ë˜ ì½”ë“œì—ì„œëŠ” ë‹¨ìˆœíˆ 'Generate multiple search queries related to: {original_query}'ë¼ëŠ” í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ë¹„êµì  ê´‘ë²”ìœ„í•œ ì¿¼ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê²°ê³¼ë¥¼ ì¢íˆê¸° ìœ„í•´, ë‚˜ëŠ” í”„ë¡¬í”„íŠ¸ì— ë‹¤ìŒê³¼ ê°™ì€ ì§€ì‹œë¬¸ì„ ì¶”ê°€í•˜ë„ë¡ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤: 'When creating queries, please refine or add closely related contextual information, without significantly altering the original query's meaning.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_generator(original_query: dict) -> list[str]:\n",
    "    query = original_query.get(\"query\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"),\n",
    "        (\"user\", \"Generate mnultiple search queries related to: {original_query}. When creating queries, please refine or add closely related contextual information in Korean, without significantly altering the original query's meaning\"),\n",
    "        (\"user\", \"OUTPUT (3 queries):\"),\n",
    "    ])\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=LLM_MODEL_OPENAI)\n",
    "\n",
    "    query_generator_chain = (\n",
    "        prompt | model | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "    )\n",
    "\n",
    "    queries = query_generator_chain.invoke({\"original_query\": query})\n",
    "\n",
    "    queries.insert(0, \"0. \" + query)\n",
    "\n",
    "    print(\"Generated queries:\\n\", \"\\n\".join(queries))\n",
    "\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë²¡í„° ê²€ìƒ‰ì€ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤:\n",
    "\n",
    "1. RunnableLambda(query_generator)ëŠ” ì•ì„œ ì„¤ëª…í•œ ëŒ€ë¡œ ìœ ì‚¬í•œ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "2. retriever.map()ì„ ì‚¬ìš©í•˜ì—¬ query_generatorê°€ ìƒì„±í•œ ë„¤ ê°œì˜ ìœ ì‚¬í•œ ì¿¼ë¦¬ì™€ ì›ë˜ ì¿¼ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ ê° ì¿¼ë¦¬ì— ëŒ€í•´ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. map() í•¨ìˆ˜ëŠ” ê° ì¿¼ë¦¬ì— ëŒ€í•´ ë‹¤ì„¯ ê°œì˜ ì¡°ê°ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "3. ë§ˆì§€ë§‰ìœ¼ë¡œ, reciprocal_rank_fusionì„ ì ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ì¬ì •ë ¬í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ìì„¸íˆ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_retriever(query: str) -> list[Document]:\n",
    "    retriever = create_retriever(search_type=\"similarity\", kwargs={\"k\": TOP_K})\n",
    "\n",
    "    chain = (\n",
    "        {\"query\": itemgetter(\"query\")}\n",
    "        | RunnableLambda(query_generator)\n",
    "        | retriever.map()\n",
    "        | reciprocal_rank_fusion\n",
    "    )\n",
    "\n",
    "    result = chain.invoke({\"query\": query})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query: str):\n",
    "    model = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model=LLM_MODEL_OPENAI\n",
    "    )\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Please answer the [question] using only the following [information] in Korean. If there is no [information] available to answer the question, do not force an answer.\n",
    "\n",
    "            Information: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            Final answer:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | RunnableLambda(rrf_retriever),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | RunnablePassthrough.assign(\n",
    "            context=itemgetter(\"context\")\n",
    "        )\n",
    "        | {\n",
    "            \"response\": prompt | model | StrOutputParser(),\n",
    "            \"context\": itemgetter(\"context\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = chain.invoke({\"question\": query})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document:  11  docs\n",
      "Generated queries:\n",
      " 0. ê°œì¸í‡´ì§ì—°ê¸ˆ\n",
      "1. ê°œì¸í‡´ì§ì—°ê¸ˆ ê°€ì… ë°©ë²• ë° ì ˆì°¨\n",
      "2. ê°œì¸í‡´ì§ì—°ê¸ˆ ì„¸ì œ í˜œíƒê³¼ ì ˆì„¸ ë°©ë²•\n",
      "3. ê°œì¸í‡´ì§ì—°ê¸ˆ ìš´ìš© ì „ëµ ë° ì¶”ì²œ ìƒí’ˆ\n",
      "Reranked documents:  3\n",
      "---\n",
      "Docs:  10/118. ìƒí•´ë³´í—˜ 1)ê°€ì…ëŒ€ìƒ : ì „ ì§ì› (ì„ì›ìƒí•´ë³´í—˜ ë³„ë„ ê°€ì…) 2) ê°€ì…ë‚´ìš© : å…¨ ì§ì›ì˜ ë¶ˆì˜ì˜ ì‚¬ê³  ë° ì§ˆë³‘ì— ëŒ€ë¹„í•˜ê¸° ìœ„í•´ íšŒì‚¬ì—ì„œ ì¼ê´„ì ìœ¼ë¡œ\n",
      "RRF score:  0.16137431484139575\n",
      "---\n",
      "Docs:  10. ì£¼íƒ ìê¸ˆ ëŒ€ì¶œ ì œë„ 1)ëŒ€ì¶œ ë‚´ìš© : ë³¸ì¸ì˜ ê±°ì£¼ ëª©ì ìœ¼ë¡œ ì£¼íƒ êµ¬ì… í˜¹ì€ ì„ì°¨ ê³„ì•½ ì‹œ ëŒ€ì¶œ ì§€ì› (ë¬´ì£¼íƒìì— í•œí•¨) 2)ëŒ€ì¶œ í•œë„ : 5ì²œë§Œì› 3)ëŒ€ì¶œ ê¸ˆë¦¬ :\n",
      "RRF score:  0.08068715742069787\n",
      "---\n",
      "Docs:  êµ¬ ë¶„ê²½ì¡°ê¸ˆ (ì›”ê¸°ë³¸ê¸‰ ê¸°ì¤€)íœ´ê°€í™”í™˜ì¥ë¡€ìš©í’ˆ ì¥ë¡€ì¸ë ¥ ê²°í˜¼ë³¸ì¸ 100% 5ì¼â—‹ ìë…€ 100% 2ì¼â—‹ í˜•ì œÂ·ìë§¤(ë³¸ì¸,ë°°ìš°ì ) 50% 1ì¼ íšŒê°‘ë³¸ì¸/ë°°ìš°ì 100% 1ì¼ ë¶€ëª¨(ë³¸\n",
      "RRF score:  0.08068715742069787\n",
      "---\n",
      "Answer:ê°œì¸í‡´ì§ì—°ê¸ˆì— ëŒ€í•œ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1) ì§€ì› ë‚´ìš© : ì„ì§ì›ì˜ ë…¸í›„ ìƒí™œ ì•ˆì •ì„ ìœ„í•˜ì—¬ ê°œì¸í‡´ì§ì—°ê¸ˆ ì§€ì›\n",
      "2) ì§€ì› ê¸ˆì•¡ : ì„ì§ì›ì´ ì ë¦½í•˜ëŠ” ê¸ˆì•¡ê³¼ ë™ì¼í•˜ê²Œ íšŒì‚¬ ì§€ì›ë¶„ ì ë¦½í•¨\n",
      "ë‹¨, ìµœëŒ€ æœˆ 25ë§Œì› í•œë„ë¡œ ì§€ì›í•¨\n"
     ]
    }
   ],
   "source": [
    "result = query(\"ê°œì¸í‡´ì§ì—°ê¸ˆ\")\n",
    "print('---\\nAnswer:' + result['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
