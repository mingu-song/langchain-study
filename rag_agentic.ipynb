{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Agentic RAG using Langchain\n",
    "* https://medium.com/the-ai-forum/implementing-agentic-rag-using-langchain-b22af7f6a3b5\n",
    "* 아... 모르겠음 ㅋ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search langchain-community\n",
    "!pip install -qU faiss-cpu pymupdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"agentic-rag-{uuid4().hex[0:8]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "docs = ArxivLoader(query=\"Retrieval Augmented Generation\", max_results=5).load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = faiss_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following context to answer the user's query in Korean. If you cannot answer the question, please respond with 'I don't know'.\\n\\nQuestion:\\n{question}\\n\\nContext:\\n{context}\\n\"))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = \"\"\"\\\n",
    "Use the following context to answer the user's query in Korean. If you cannot answer the question, please respond with 'I don't know'.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(rag_prompt)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "open_chat_model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(itemgetter('question'))\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x3619a3550>),\n",
       "  question: RunnableLambda(itemgetter('question'))\n",
       "}\n",
       "| RunnableAssign(mapper={\n",
       "    context: RunnableLambda(itemgetter('context'))\n",
       "  })\n",
       "| {\n",
       "    response: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following context to answer the user's query in Korean. If you cannot answer the question, please respond with 'I don't know'.\\n\\nQuestion:\\n{question}\\n\\nContext:\\n{context}\\n\"))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x127da9720>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x127dab4f0>, model_name='gpt-4o', openai_api_key=SecretStr('**********'), openai_proxy=''),\n",
       "    context: RunnableLambda(itemgetter('context'))\n",
       "  }"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\") }\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": template | open_chat_model, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': AIMessage(content='RAG는 Retrieval-Augmented Generation의 약자로, 정보 검색 프로세스를 통합하여 대형 언어 모델(LLM)의 정적 한계를 해결하는 방법론입니다. 이는 최신 외부 정보를 동적으로 통합함으로써 LLM의 정확성과 신뢰성을 향상시키는 것을 목표로 합니다. RAG는 주로 텍스트 도메인에 초점을 맞추며, LLM이 실제 데이터를 사용하여 더 정확하고 신뢰할 수 있는 출력을 생성하도록 돕습니다. RAG는 사전 검색, 검색, 검색 후 처리, 생성의 네 가지 범주로 조직되며, 각 단계에서 성능을 최적화하기 위한 다양한 기법과 평가 방법이 포함됩니다.', response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 2747, 'total_tokens': 2900}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_43dfabdef1', 'finish_reason': 'stop', 'logprobs': None}, id='run-cff8b354-2b4a-4832-b66b-da3b0ca174bc-0'),\n",
       " 'context': [Document(page_content='RGB (Chen et al., 2023b)\\nImpact of RAG on LLMs\\nNoise Robustness\\nAccuracy\\nSynthetic Dataset including English and Chinese\\nNegative Rejection\\nRejection Rate\\nInformation Integration\\nAccuracy\\nCounterfactual Robustness\\nError Detection Rate\\nError Correction Rate\\nTable 3: The Comparison of Different RAG Evaluation Frameworks\\n8.1\\nRetrieval-based Aspect\\nIn information retrieval, the quality of search re-\\nsults is typically evaluated using standard metrics\\nsuch as Mean Average Precision (MAP), Precision,\\nReciprocal Rank, and Normalized Discounted Cu-\\nmulative Gain (NDCG) (Radlinski and Craswell,\\n2010; Reimers and Gurevych, 2019; Nogueira et al.,\\n2019). These metrics primarily assess the relevance\\nof retrieved documents to a given query.\\nRetrieval-based Metrics in RAG focus on the ef-\\nfectiveness of retrieving relevant information to\\nsupport generation tasks.\\nThese include Accu-\\nracy, which measures the precision of retrieved\\ndocuments in providing correct information for an-\\nswering queries, and Rejection Rate (Chen et al.,\\n2023b), assessing a system’s ability to decline an-\\nswering when no relevant information is found.\\nAdditionally, Error Detection Rate (Chen et al.,\\n2023b) evaluates the model’s capability to identify\\nand disregard incorrect or misleading information\\nfrom retrieved documents. Context Relevance is\\nanother essential metric, assessing the pertinence', metadata={'Published': '2024-04-17', 'Title': 'A Survey on Retrieval-Augmented Text Generation for Large Language Models', 'Authors': 'Yizheng Huang, Jimmy Huang', 'Summary': \"Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\\nlearning advancements to address the static limitations of large language\\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\\ninformation. This methodology, focusing primarily on the text domain, provides\\na cost-effective solution to the generation of plausible but incorrect\\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\\noutputs through the use of real-world data. As RAG grows in complexity and\\nincorporates multiple concepts that can influence its performance, this paper\\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\\npost-retrieval, and generation, offering a detailed perspective from the\\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\\nprogression through the analysis of significant studies. Additionally, the\\npaper introduces evaluation methods for RAG, addressing the challenges faced\\nand proposing future research directions. By offering an organized framework\\nand categorization, the study aims to consolidate existing research on RAG,\\nclarify its technological underpinnings, and highlight its potential to broaden\\nthe adaptability and applications of LLMs.\"}),\n",
       "  Document(page_content='characteristics. We believe designing domain-specific RAG\\ntechniques will significantly benefit broader applications.\\n4) Efficient Deployment and Processing:\\nThere exist\\nseveral deployment solutions for query-based RAG with\\nLLMs, such as LangChain [332], LLAMA-Index [131], and\\nPipeRAG [333]. However, for other RAG foundations and/or\\ngeneration tasks, there lacks a plug-and-play solution. Besides,\\n16\\ndue to retrieval overhead and increasing complexities in re-\\ntrievers and generators, achieving efficient RAG is still chal-\\nlenging and necessitates further system-level optimizations.\\n5) Incorporating Long-tail and Real-time Knowledge:\\nWhile a key motivation of RAG is to harness real-time and\\nlong-tail knowledge, few studies have explored the pipeline\\nfor knowledge updating and expansion. Many existing works\\nuse merely the generators’ training data as retrieval sources,\\nneglecting the dynamic and flexible information that retrieval\\ncould offer. As a consequence, there is a growing research on\\ndesigning RAG systems with continuously updated knowledge\\nand flexible sources. We also expect RAG to step further,\\nadapting to personalized information in today’s web service.\\n6) Combined with Other Techniques: RAG is orthogonal\\nto other techniques that also aim to improve AIGC effec-\\ntiveness, such as fine-tuning, reinforcement learning, chain-\\nof-thought, and agent-based generation. The combining of\\nthese methods is still in its early stages, calling for further\\nresearch to fully exploit their potential through novel algorithm', metadata={'Published': '2024-05-02', 'Title': 'Retrieval-Augmented Generation for AI-Generated Content: A Survey', 'Authors': 'Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui', 'Summary': 'Advancements in model algorithms, the growth of foundational models, and\\naccess to high-quality datasets have propelled the evolution of Artificial\\nIntelligence Generated Content (AIGC). Despite its notable successes, AIGC\\nstill faces hurdles such as updating knowledge, handling long-tail data,\\nmitigating data leakage, and managing high training and inference costs.\\nRetrieval-Augmented Generation (RAG) has recently emerged as a paradigm to\\naddress such challenges. In particular, RAG introduces the information\\nretrieval process, which enhances the generation process by retrieving relevant\\nobjects from available data stores, leading to higher accuracy and better\\nrobustness. In this paper, we comprehensively review existing efforts that\\nintegrate RAG technique into AIGC scenarios. We first classify RAG foundations\\naccording to how the retriever augments the generator, distilling the\\nfundamental abstractions of the augmentation methodologies for various\\nretrievers and generators. This unified perspective encompasses all RAG\\nscenarios, illuminating advancements and pivotal technologies that help with\\npotential future progress. We also summarize additional enhancements methods\\nfor RAG, facilitating effective engineering and implementation of RAG systems.\\nThen from another view, we survey on practical applications of RAG across\\ndifferent modalities and tasks, offering valuable references for researchers\\nand practitioners. Furthermore, we introduce the benchmarks for RAG, discuss\\nthe limitations of current RAG systems, and suggest potential directions for\\nfuture research. Github: https://github.com/PKU-DAIR/RAG-Survey.'}),\n",
       "  Document(page_content='VGNMN‡\\nVidIL†‡\\nRAG-Driver‡\\nAnimate-A-Story†§\\nRAG for Science\\nRAG for Audio\\nDrug Discovery\\nBiomedical Informatics Enhancement\\nMath Applications\\nAudio Generation\\nAudio Captioning\\nRetMol†§\\nPromptDiff†‡\\nPoET‡\\nChat-Orthopedist†§\\nBIOREADER†\\nMedWriter‡\\nQARAG†‡\\nLeanDojo‡\\nRAG-for-math-QA†‡\\nRe-AudioLDM§\\nMake-An-Audio†§\\nRECAP‡§\\nQuery-based\\nLatent-based\\nLogit-based\\nSpeculative\\nQuery+Latent\\nLatent+Logit\\n† Input\\n‡ Retriever\\n§ Generator\\n∥Output\\n¶ Pipeline\\ngenerator (BART) to optimize performance for the end-to-end\\nquestion-answering task and to facilitate domain adaptation.\\nMultiHop-RAG [186] extracts and aggregates information\\nfrom distinct documents, providing the generator with the\\nnecessary context for definitive query answers.\\n2) Fact Verification: Fact Verification typically refers to\\ndetermining whether a given natural language text and a\\nrelated claim or assertion match the facts in the text. CON-\\nCRETE [187] leverages cross-lingual retrieval mechanisms to\\ntap into a wealth of multilingual evidence, effectively bridging\\nthe gap in resources for languages that are underrepresented\\nin fact-checking datasets. Hagstr¨\\nom et al. [188] proved on', metadata={'Published': '2024-05-02', 'Title': 'Retrieval-Augmented Generation for AI-Generated Content: A Survey', 'Authors': 'Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui', 'Summary': 'Advancements in model algorithms, the growth of foundational models, and\\naccess to high-quality datasets have propelled the evolution of Artificial\\nIntelligence Generated Content (AIGC). Despite its notable successes, AIGC\\nstill faces hurdles such as updating knowledge, handling long-tail data,\\nmitigating data leakage, and managing high training and inference costs.\\nRetrieval-Augmented Generation (RAG) has recently emerged as a paradigm to\\naddress such challenges. In particular, RAG introduces the information\\nretrieval process, which enhances the generation process by retrieving relevant\\nobjects from available data stores, leading to higher accuracy and better\\nrobustness. In this paper, we comprehensively review existing efforts that\\nintegrate RAG technique into AIGC scenarios. We first classify RAG foundations\\naccording to how the retriever augments the generator, distilling the\\nfundamental abstractions of the augmentation methodologies for various\\nretrievers and generators. This unified perspective encompasses all RAG\\nscenarios, illuminating advancements and pivotal technologies that help with\\npotential future progress. We also summarize additional enhancements methods\\nfor RAG, facilitating effective engineering and implementation of RAG systems.\\nThen from another view, we survey on practical applications of RAG across\\ndifferent modalities and tasks, offering valuable references for researchers\\nand practitioners. Furthermore, we introduce the benchmarks for RAG, discuss\\nthe limitations of current RAG systems, and suggest potential directions for\\nfuture research. Github: https://github.com/PKU-DAIR/RAG-Survey.'}),\n",
       "  Document(page_content='ing the complex challenges faced by RAG systems\\nin practical applications. The continuous devel-\\nopment of evaluation frameworks and metrics is\\ncrucial for advancing the field, broadening the ap-\\nplicability of RAG systems, and ensuring they meet\\nthe demands of a complex and evolving informa-\\ntion landscape.\\n9\\nEvaluation Framework\\nAspects\\nMethods\\nMetrics\\nDatasets\\nRAGAS (Shahul et al., 2023)\\nQuality of RAG Systems\\nContext Relevance\\nExtracted Sentences / Total Sentences\\nWikiEval 4\\nAnswer Relevance\\nAverage Cosine Similarity\\nFaithfulness\\nSupported Statements / Total Statements\\nARES (Saad-Falcon et al., 2023)\\nImproving RAGAS\\nContext Relevance\\nConfidence Intervals\\nKILT (Petroni et al., 2021)\\nSuperGLUE (Wang et al., 2019)\\nAnswer Relevance\\nAnswer Faithfulness\\nRECALL (Liu et al., 2023)\\nCounterfactual Robustness\\nResponse Quality\\nAccuracy (QA)\\nBLEU, ROUGE-L (Generation)\\nEventKG (Gottschalk and Demidova, 2018)\\nUJ (Huang et al., 2022)\\nRobustness\\nMisleading Rate (QA)\\nMistake Reappearance Rate (Generation)\\nRGB (Chen et al., 2023b)\\nImpact of RAG on LLMs\\nNoise Robustness\\nAccuracy\\nSynthetic Dataset including English and Chinese\\nNegative Rejection', metadata={'Published': '2024-04-17', 'Title': 'A Survey on Retrieval-Augmented Text Generation for Large Language Models', 'Authors': 'Yizheng Huang, Jimmy Huang', 'Summary': \"Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\\nlearning advancements to address the static limitations of large language\\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\\ninformation. This methodology, focusing primarily on the text domain, provides\\na cost-effective solution to the generation of plausible but incorrect\\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\\noutputs through the use of real-world data. As RAG grows in complexity and\\nincorporates multiple concepts that can influence its performance, this paper\\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\\npost-retrieval, and generation, offering a detailed perspective from the\\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\\nprogression through the analysis of significant studies. Additionally, the\\npaper introduces evaluation methods for RAG, addressing the challenges faced\\nand proposing future research directions. By offering an organized framework\\nand categorization, the study aims to consolidate existing research on RAG,\\nclarify its technological underpinnings, and highlight its potential to broaden\\nthe adaptability and applications of LLMs.\"})]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.ainvoke({\"question\": \"RAG 가 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duck Duck Go Web Search\n",
    "* https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tool_belt = [\n",
    "    DuckDuckGoSearchRun(),\n",
    "    ArxivQueryRun(),\n",
    "]\n",
    "\n",
    "tool_executor = ToolExecutor(tool_belt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "functions = [convert_to_openai_function(f) for f in tool_belt]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_tool(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "    )\n",
    "    response = tool_executor.invoke(action)\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "\n",
    "    return {\"messages\": [function_message]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': agent(recurse=True), 'action': action(recurse=True)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build workflow\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "workflow.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    \n",
    "    return \"continue\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompiledStateGraph(nodes={'__start__': PregelNode(config={'tags': ['langsmith:hidden']}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<messages>(recurse=True, writes=[ChannelWriteEntry(channel='messages', value=<object object at 0x1048dfd70>, skip_none=False, mapper=_get_state_key(recurse=False))]), ChannelWrite<start:agent>(recurse=True, writes=[ChannelWriteEntry(channel='start:agent', value='__start__', skip_none=False, mapper=None)])]), 'agent': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['start:agent', 'action'], mapper=functools.partial(<function _coerce_state at 0x110347760>, <class '__main__.AgentState'>), writers=[ChannelWrite<agent,messages>(recurse=True, writes=[ChannelWriteEntry(channel='agent', value='agent', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x1048dfd70>, skip_none=False, mapper=_get_state_key(recurse=False))]), _route(recurse=True, _is_channel_writer=True)]), 'action': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['branch:agent:should_continue:action'], mapper=functools.partial(<function _coerce_state at 0x110347760>, <class '__main__.AgentState'>), writers=[ChannelWrite<action,messages>(recurse=True, writes=[ChannelWriteEntry(channel='action', value='action', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x1048dfd70>, skip_none=False, mapper=_get_state_key(recurse=False))])])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x350361b10>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x350360400>, 'agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x350363370>, 'action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x350360280>, 'start:agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x3503602e0>, 'branch:agent:should_continue:action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x350361ba0>}, auto_validate=False, stream_mode='updates', output_channels=['messages'], stream_channels=['messages'], input_channels='__start__', builder=<langgraph.graph.state.StateGraph object at 0x350363700>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = workflow.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG는 \"Retrieval-Augmented Generation\"의 약자로, 대형 언어 모델(LLM)에서 정보를 생성할 때 외부 데이터베이스나 문서에서 관련 정보를 검색하여 이를 활용하는 방법론을 의미합니다. RAG는 주로 다음과 같은 두 가지 주요 단계를 포함합니다:\n",
      "\n",
      "1. **Retrieval (검색)**: 사용자의 질문이나 요청에 대해 관련된 문서를 외부 데이터베이스나 문서 집합에서 검색합니다. 이 단계에서는 검색 엔진이나 정보 검색 알고리즘을 사용하여 관련성이 높은 문서를 찾습니다.\n",
      "\n",
      "2. **Generation (생성)**: 검색된 문서를 바탕으로 사용자의 질문에 대한 답변을 생성합니다. 이 단계에서는 대형 언어 모델이 검색된 정보를 활용하여 보다 정확하고 풍부한 답변을 생성합니다.\n",
      "\n",
      "RAG는 특히 다음과 같은 상황에서 유용합니다:\n",
      "- **정보의 최신성**: 모델이 학습된 이후에 발생한 최신 정보를 반영할 수 있습니다.\n",
      "- **전문성**: 특정 도메인에 대한 깊이 있는 지식이 필요할 때, 해당 도메인의 문서를 검색하여 보다 정확한 답변을 제공할 수 있습니다.\n",
      "- **모델의 한계 보완**: 모델이 학습 데이터에 포함되지 않은 정보를 제공해야 할 때, 외부 문서를 검색하여 이를 보완할 수 있습니다.\n",
      "\n",
      "RAG는 최근 몇 년간 언급되기 시작했으며, 특히 2020년대 초반부터 주목받기 시작했습니다. 이는 대형 언어 모델의 성능이 향상됨에 따라, 단순한 텍스트 생성에서 벗어나 보다 정확하고 신뢰할 수 있는 정보를 제공하기 위한 방법론으로 발전해왔기 때문입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"LLM에서 RAG는 무엇인가요? 언제 언급되기 시작했나요?\")]}\n",
    "response = app.invoke(inputs)\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation (RAG) is a topic that has been explored by multiple researchers. Here are some of the main authors from notable papers on the subject:\n",
      "\n",
      "1. **Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu** - Authors of \"A Survey on Retrieval-Augmented Text Generation\" (Published: 2022-02-13).\n",
      "2. **Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang** - Authors of \"DuetRAG: Collaborative Retrieval-Augmented Generation\" (Published: 2024-05-12).\n",
      "3. **Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi** - Authors of \"Context Tuning for Retrieval Augmented Generation\" (Published: 2023-12-09).\n",
      "\n",
      "To determine the educational background of these authors, I would need to look up each individual author. If you have a specific author in mind, please let me know, and I can search for their educational background.\n"
     ]
    }
   ],
   "source": [
    "question = \"Retrieval Augmented Generation 의 메인 저자는 누구인가요? 그리고 어떤 대학을 나왔나요\"\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
    "response = app.invoke(inputs)\n",
    "print(response['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
